{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pdfs from a directory\n",
    "pdf_loader = PyPDFDirectoryLoader('Data')\n",
    "pdfs = pdf_loader.load()\n",
    "#pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning Function\n",
    "for i, pdf in enumerate(pdfs):\n",
    "    page = pdf.page_content\n",
    "    page = re.sub(r'([\\n]+)([0-9]+)', '', page)\n",
    "    page = re.sub(r'([0-9]+) [.]', '', page)\n",
    "    page = re.sub(r'([\\n]+)', '', page)\n",
    "    page = page.replace('•', '')\n",
    "    page = re.sub(' +', ' ', page)\n",
    "    page = page.replace(\"•\", \"\")\n",
    "    page = page.replace('\"', \"\")\n",
    "    if page[0].isdigit():\n",
    "        page = page[1:]\n",
    "    page = page.strip()\n",
    "    pdfs[i].page_content = page\n",
    "#pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial 174 documents were split into 2605 documents\n"
     ]
    }
   ],
   "source": [
    "# Split the documents into smaller , managable chuncks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "document_chunks = text_splitter.split_documents(pdfs)\n",
    "print(f\"The initial {len(pdfs)} documents were split into {len(document_chunks)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import transformers\n",
    "\n",
    "# print(torch.__version__)  # Check torch version\n",
    "# print(transformers.__version__)  # Check transformers version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding configuration for nomic-embed-text saved.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "import pickle\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Save the embedding model configuration\n",
    "embedding_config = {\n",
    "    \"model\": \"nomic-embed-text\",\n",
    "}\n",
    "\n",
    "# Save the configuration to a pickle file\n",
    "with open('Data/embedding_config_nomic.pkl', 'wb') as f:\n",
    "    pickle.dump(embedding_config, f)\n",
    "\n",
    "print(\"Embedding configuration for nomic-embed-text saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "# import pickle\n",
    "\n",
    "\n",
    "# embeddings = HuggingFaceBgeEmbeddings(\n",
    "#     model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "#     model_kwargs={'device': 'cpu'},\n",
    "#     encode_kwargs={'normalize_embeddings': True}\n",
    "# )\n",
    "\n",
    "# # Save the embedding model configuration separately (e.g., in a pickle file)\n",
    "# embedding_config = {\n",
    "#     \"model_name\": \"BAAI/bge-small-en-v1.5\",\n",
    "#     \"model_kwargs\": {'device': 'cpu'},\n",
    "#     \"encode_kwargs\": {'normalize_embeddings': True}\n",
    "# }\n",
    "# with open('Data/embedding_config.pkl', 'wb') as f:\n",
    "#     pickle.dump(embedding_config, f)\n",
    "\n",
    "# print(\"FAISS index and embedding configuration saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2605"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the embeddings into Faiss index\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(document_chunks, embeddings)\n",
    "\n",
    "db.index.ntotal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the db to disk\n",
    "db.save_local('Data/faiss_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2605"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the db from disk\n",
    "db = FAISS.load_local(\n",
    "    folder_path='Data/faiss_index',\n",
    "    embeddings=embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "db.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"*\"How do I start the powerstation?\"™\"\n",
    "query = \"do i get travel allownance?\"\n",
    "relevant_documents = db.similarity_search(query)\n",
    "\n",
    "# for i in relevant_documents:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use similarity searching algorithm and return 3 most relevant chunks.\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "#retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup an LLM for text generation \n",
    "# import ollama\n",
    "# from langchain_ollama import OllamaLLM\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "# from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are a friendly senior Human Resource(HR) personnel. \n",
    "You will be given a question from an employee regarding their queries related to HR-Policies. \n",
    "Your task is to understand the employee's question first thoroughly, then based on the context provided to you, \n",
    "answer the employee in 3-4 concise sentences. \n",
    "If you don't know the answer to the employee's question, say \"I don't know the answer to your question, \n",
    "Please contact the focal HR personnel in your department.\"\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Your Helpful Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_template(Prompt_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the ollama model\n",
    "model = Ollama(model=\"llama3.2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retrieval QA chain\n",
    "retrievalQA = RetrievalQA. from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": chat_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the retrieval QA chain\n",
    "user_query = \"as an AVP executive what is my purchase price  entitlementfor vehicles for markup free loan?\"\n",
    "answer = retrievalQA. invoke({\"query\": user_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as an AVP executive what is my purchase price  entitlementfor vehicles for markup free loan?\n",
      "As an AVP Executive, your purchase price entitlement for a markup-free car loan would be fixed in January each year and listed under the \"Grade Vehicle Entitlement\" list. I don't know the exact figures, but it's mentioned that this information will be provided annually. You can check with our HR department or finance team to get the most up-to-date information on your entitlement. They should be able to provide you with more details on what vehicles are included in the list and how much of a loan is available.\n",
      "Further information in HR Manual: [35, 35, 65]\n"
     ]
    }
   ],
   "source": [
    "# Print the answer\n",
    "print(answer['query'])\n",
    "print(answer['result'])\n",
    "\n",
    "pages = []\n",
    "for i in answer['source_documents']:\n",
    "    pages.append(i.metadata['page'])\n",
    "pages.sort()\n",
    "print(\"Further information in HR Manual: {}\".format(pages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
